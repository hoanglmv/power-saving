{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Tải các thư viện ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "# Cấu hình hiển thị\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ac6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2 (train.ipynb - SỬA LỖI CHO CSV): Tải dữ liệu đã xử lý ---\n",
    "file_name = 'kpi_processed.csv' # <-- Đã đổi tên file\n",
    "try:\n",
    "    # *** THAY ĐỔI QUAN TRỌNG ***\n",
    "    # Chúng ta dùng pd.read_csv và thêm 2 tham số:\n",
    "    # 1. index_col=0: Báo cho Pandas rằng cột đầu tiên (cột 0) là index.\n",
    "    # 2. parse_dates=True: Báo cho Pandas chuyển đổi index đó thành Datetime.\n",
    "    df = pd.read_csv(file_name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    print(f\"Tải file '{file_name}' thành công.\")\n",
    "    print(f\"Dữ liệu có {len(df)} hàng, kéo dài từ {df.index.min()} đến {df.index.max()}.\")\n",
    "    print(\"\\nThông tin dữ liệu (df.info()):\")\n",
    "    df.info() \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LỖI: Không thể tải file '{file_name}'.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 3: Xử lý dữ liệu (Quan trọng!) ---\n",
    "# Mục tiêu: Đảm bảo MỌI cell_name đều có đầy đủ 15 phút, không bị ngắt quãng.\n",
    "# Nếu thiếu, chúng ta sẽ lấp đầy bằng 0.\n",
    "if 'df' in locals():\n",
    "    # Xác định các cột KPI để dự đoán\n",
    "    FEATURE_COLS = ['ps_traffic_mb', 'avg_rrc_connected_user', 'prb_dl_used', 'prb_dl_available_total', 'prb_utilization']\n",
    "    N_FEATURES = len(FEATURE_COLS)\n",
    "    print(f\"Dự đoán {N_FEATURES} features: {FEATURE_COLS}\")\n",
    "    # Tạo ra một DataFrame đầy đủ cho MỌI cell\n",
    "    print(\"Đang xử lý: Đảm bảo dữ liệu 15 phút đầy đủ cho mỗi cell...\")\n",
    "    # Lấy tất cả các cell độc nhất\n",
    "    all_cells = df['cell_name'].unique()\n",
    "    # Tạo ra một index thời gian 15 phút đầy đủ từ đầu đến cuối\n",
    "    # (Sửa lỗi cảnh báo: '15T' -> '15min')\n",
    "    full_time_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='15min')\n",
    "    # Tạo MultiIndex (cell_name, timestamp)\n",
    "    multi_index = pd.MultiIndex.from_product([all_cells, full_time_index], names=['cell_name', 'timestamp'])\n",
    "    # Lấy các cột dữ liệu\n",
    "    df_data_only = df[FEATURE_COLS + ['cell_name']]\n",
    "    # Đưa timestamp ra thành một cột\n",
    "    df_data_only = df_data_only.reset_index()\n",
    "    # Xử lý các (cell_name, timestamp) bị trùng lặp\n",
    "    print(f\"Dữ liệu trước khi xử lý trùng lặp: {len(df_data_only)} hàng\")\n",
    "    df_grouped_unique = df_data_only.groupby(['cell_name', 'timestamp']).mean()\n",
    "    print(f\"Dữ liệu sau khi xử lý trùng lặp (groupby.mean): {len(df_grouped_unique)} hàng\")\n",
    "    # Reindex với MultiIndex đầy đủ, điền 0 vào các chỗ thiếu\n",
    "    df_full = df_grouped_unique.reindex(multi_index, fill_value=0)\n",
    "    # *** (BƯỚC SỬA LỖI KEYERROR) ***\n",
    "    # 1. Reset TẤT CẢ các cấp index ('cell_name', 'timestamp') ra thành cột\n",
    "    df_full = df_full.reset_index() \n",
    "    # 2. Đặt cột 'timestamp' làm index mới.\n",
    "    # 'cell_name' bây giờ sẽ tự động là một cột, chính xác như chúng ta muốn.\n",
    "    df_full = df_full.set_index('timestamp')\n",
    "    # *** (KẾT THÚC SỬA LỖI) ***\n",
    "    print(f\"Dữ liệu gốc có {len(df)} hàng.\")\n",
    "    print(f\"Dữ liệu đã điền đầy đủ (sau reindex) có {len(df_full)} hàng.\")\n",
    "    print(\"Hoàn tất xử lý.\")\n",
    "    print(\"\\n5 dòng đầu của dữ liệu đã xử lý đầy đủ:\")\n",
    "    print(df_full.head())\n",
    "else:\n",
    "    print(\"Lỗi: DataFrame 'df' không tồn tại. Vui lòng chạy lại Cell 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: \n",
    "# Data của chúng ta có tần suất 15 phút (4 mẫu/giờ)\n",
    "TIMESTEPS_PER_HOUR = 4\n",
    "TIMESTEPS_PER_DAY = 24 * TIMESTEPS_PER_HOUR # 96\n",
    "# (THAY ĐỔI QUAN TRỌNG TẠI ĐÂY) \n",
    "# Input: 2 ngày\n",
    "INPUT_DAYS = 2\n",
    "INPUT_STEPS = INPUT_DAYS * TIMESTEPS_PER_DAY # 2 * 96 = 192\n",
    "# Output: 1 ngày\n",
    "OUTPUT_DAYS = 1\n",
    "OUTPUT_STEPS = OUTPUT_DAYS * TIMESTEPS_PER_DAY # 1 * 96 = 96\n",
    "# *** (KẾT THÚC THAY ĐỔI) ***\n",
    "\n",
    "# Yêu cầu dữ liệu tối thiểu = 192 + 96 = 276 steps (28 ngày)\n",
    "print(f\"Cấu hình mô hình: Input {INPUT_STEPS} bước (2 ngày), Output {OUTPUT_STEPS} bước (1 ngày).\")\n",
    "print(f\"Yêu cầu dữ liệu tối thiểu cho 1 mẫu: 276 bước (28 ngày)\")\n",
    "\n",
    "# TÁCH DỮ LIỆU THEO THỜI GIAN (70/20/10)\n",
    "if 'df_full' in locals():\n",
    "    total_duration = df_full.index.max() - df_full.index.min()\n",
    "    \n",
    "    # 70% cho Train\n",
    "    train_end_time = df_full.index.min() + total_duration * 0.7\n",
    "    \n",
    "    # Thêm 20% cho Validation (tổng 90%)\n",
    "    val_end_time = df_full.index.min() + total_duration * 0.9\n",
    "    \n",
    "    # 10% còn lại cho Test\n",
    "\n",
    "    print(f\"\\nMốc Train (70%):  Kết thúc lúc {train_end_time}\")\n",
    "    print(f\"Mốc Val (20%):    Kết thúc lúc {val_end_time} (Bắt đầu từ {train_end_time})\")\n",
    "    print(f\"Mốc Test (10%):   Bắt đầu từ  {val_end_time}\")\n",
    "\n",
    "    # Tách\n",
    "    train_df = df_full[df_full.index < train_end_time]\n",
    "    val_df = df_full[(df_full.index >= train_end_time) & (df_full.index < val_end_time)]\n",
    "    test_df = df_full[df_full.index >= val_end_time]\n",
    "    \n",
    "    print(f\"\\nKích thước tập Train: {train_df.shape}\")\n",
    "    print(f\"Kích thước tập Val:   {val_df.shape}\")\n",
    "    print(f\"Kích thước tập Test:  {test_df.shape}\")\n",
    "else:\n",
    "    print(\"Lỗi: DataFrame 'df_full' không tồn tại. Vui lòng chạy lại Cell 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 5: Chuẩn hóa (Scaling) Dữ liệu ---\n",
    "# Chúng ta phải fit Scaler CHỈ trên dữ liệu train để tránh rò rỉ dữ liệu\n",
    "\n",
    "# Kiểm tra xem có NaN hay Inf trong train_df không\n",
    "print(\"Kiểm tra dữ liệu bẩn:\")\n",
    "print(\"Số lượng NaN:\", train_df.isna().sum().sum())\n",
    "print(\"Số lượng Inf:\", np.isinf(train_df[FEATURE_COLS].values).sum())\n",
    "\n",
    "# Xử lý thay thế (nếu có)\n",
    "train_df = train_df.replace([np.inf, -np.inf], 0)\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "val_df = val_df.replace([np.inf, -np.inf], 0)\n",
    "val_df = val_df.fillna(0)\n",
    "\n",
    "test_df = test_df.replace([np.inf, -np.inf], 0)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "print(\"Đã làm sạch dữ liệu NaN/Inf.\")\n",
    "if 'train_df' in locals():\n",
    "    # Khởi tạo Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # 1. Fit scaler CHỈ trên dữ liệu train (chỉ các cột features)\n",
    "    # Chúng ta phải fit trên toàn bộ dữ liệu train để scaler học được min/max\n",
    "    scaler.fit(train_df[FEATURE_COLS])\n",
    "    \n",
    "    # 2. Transform cả 3 tập\n",
    "    # Lưu lại 'cell_name' để dùng cho việc nhóm\n",
    "    train_cells = train_df['cell_name']\n",
    "    val_cells = val_df['cell_name']\n",
    "    test_cells = test_df['cell_name']\n",
    "\n",
    "    # Transform\n",
    "    train_scaled_data = scaler.transform(train_df[FEATURE_COLS])\n",
    "    val_scaled_data = scaler.transform(val_df[FEATURE_COLS])\n",
    "    test_scaled_data = scaler.transform(test_df[FEATURE_COLS])\n",
    "    \n",
    "    # 3. Tạo lại DataFrame đã scale (việc này giúp nhóm dễ dàng hơn)\n",
    "    scaled_train_df = pd.DataFrame(train_scaled_data, columns=FEATURE_COLS, index=train_df.index)\n",
    "    scaled_train_df['cell_name'] = train_cells\n",
    "\n",
    "    scaled_val_df = pd.DataFrame(val_scaled_data, columns=FEATURE_COLS, index=val_df.index)\n",
    "    scaled_val_df['cell_name'] = val_cells\n",
    "\n",
    "    scaled_test_df = pd.DataFrame(test_scaled_data, columns=FEATURE_COLS, index=test_df.index)\n",
    "    scaled_test_df['cell_name'] = test_cells\n",
    "    \n",
    "    print(\"Hoàn tất scaling dữ liệu.\")\n",
    "    print(\"Dữ liệu train sau khi scale (5 dòng đầu):\")\n",
    "    print(scaled_train_df.head())\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 6: Hàm tạo cửa sổ (Windowing) ---\n",
    "# Đây là hàm quan trọng nhất:\n",
    "# Nó sẽ duyệt qua TỪNG cell_name, sau đó tạo các cặp (X, y)\n",
    "# X = 672 bước (7 ngày), y = 96 bước (1 ngày)\n",
    "\n",
    "def create_windows(data_df, input_steps, output_steps, feature_cols):\n",
    "    \"\"\"\n",
    "    Tạo các cửa sổ X (input) và y (output) từ dữ liệu đã scale,\n",
    "    nhóm theo 'cell_name'.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Nhóm dữ liệu theo từng cell\n",
    "    grouped = data_df.groupby('cell_name')\n",
    "    \n",
    "    total_cells = len(grouped)\n",
    "    print(f\"Bắt đầu tạo cửa sổ cho {total_cells} cell...\")\n",
    "    \n",
    "    cell_count = 0\n",
    "    for cell_id, cell_data in grouped:\n",
    "        cell_count += 1\n",
    "        if cell_count % 50 == 0:\n",
    "            print(f\"  ...đang xử lý cell {cell_count}/{total_cells} (ID: {cell_id})\")\n",
    "            \n",
    "        # Lấy dữ liệu số của cell này\n",
    "        cell_features = cell_data[feature_cols].values\n",
    "        \n",
    "        # Tổng số mẫu của cell này\n",
    "        total_samples = len(cell_features)\n",
    "        \n",
    "        # Tổng độ dài cần thiết cho 1 cửa sổ\n",
    "        total_window_len = input_steps + output_steps\n",
    "        \n",
    "        # Trượt cửa sổ\n",
    "        for i in range(total_samples - total_window_len + 1):\n",
    "            # i là điểm bắt đầu của input\n",
    "            input_start = i\n",
    "            input_end = i + input_steps\n",
    "            \n",
    "            # output_end là điểm kết thúc của output\n",
    "            output_end = input_end + output_steps\n",
    "            \n",
    "            # Lấy cửa sổ X và y\n",
    "            window_X = cell_features[input_start:input_end, :]\n",
    "            window_y = cell_features[input_end:output_end, :]\n",
    "            \n",
    "            X.append(window_X)\n",
    "            y.append(window_y)\n",
    "            \n",
    "    print(f\"Hoàn tất tạo cửa sổ. Đã tạo {len(X)} mẫu.\")\n",
    "    \n",
    "    # Chuyển list thành Numpy array\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Áp dụng hàm Windowing ---\n",
    "\n",
    "if 'scaled_train_df' in locals():\n",
    "    print(\"--- Đang tạo mẫu Train (X_train, y_train) ---\")\n",
    "    X_train, y_train = create_windows(scaled_train_df, INPUT_STEPS, OUTPUT_STEPS, FEATURE_COLS)\n",
    "    \n",
    "    print(\"\\n--- Đang tạo mẫu Validation (X_val, y_val) ---\")\n",
    "    X_val, y_val = create_windows(scaled_val_df, INPUT_STEPS, OUTPUT_STEPS, FEATURE_COLS)\n",
    "    \n",
    "    print(\"\\n--- Đang tạo mẫu Test (X_test, y_test) ---\")\n",
    "    X_test, y_test = create_windows(scaled_test_df, INPUT_STEPS, OUTPUT_STEPS, FEATURE_COLS)\n",
    "    \n",
    "    print(\"\\n--- Kích thước dữ liệu (Shape) ---\")\n",
    "    print(f\"X_train shape: {X_train.shape}\") # (Số mẫu, 672, N_FEATURES)\n",
    "    print(f\"y_train shape: {y_train.shape}\") # (Số mẫu, 96, N_FEATURES)\n",
    "    print(f\"X_val shape:   {X_val.shape}\")\n",
    "    print(f\"y_val shape:   {y_val.shape}\")\n",
    "    print(f\"X_test shape:  {X_test.shape}\")\n",
    "    print(f\"y_test shape:  {y_test.shape}\")\n",
    "else:\n",
    "    print(\"Lỗi: Dữ liệu đã scale không tồn tại. Vui lòng chạy lại Cell 5.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639623ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Xây dựng mô hình Transformer (Time Series) ---\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    Khối Encoder của Transformer gồm:\n",
    "    Multi-Head Attention + Normalization + Feed Forward\n",
    "    \"\"\"\n",
    "    # 1. Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs # Residual Connection (Kết nối tắt)\n",
    "\n",
    "    # 2. Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer_model(input_shape, output_steps, output_features, \n",
    "                            head_size=256, num_heads=4, ff_dim=4, \n",
    "                            num_transformer_blocks=4, mlp_units=[128], \n",
    "                            dropout=0, mlp_dropout=0):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # --- A. Feature Projection ---\n",
    "    # Chiếu dữ liệu gốc (5 features) lên không gian lớn hơn để Attention hoạt động tốt\n",
    "    # Giống như việc \"nhúng\" (embedding) trong NLP\n",
    "    x = layers.Dense(head_size)(x) \n",
    "\n",
    "    # --- B. Transformer Encoder Blocks ---\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # --- C. Output Head (Decoder đơn giản hóa) ---\n",
    "    # Lấy đặc trưng trung bình hoặc Flatten\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    \n",
    "    # Các lớp Dense (MLP) để học mối quan hệ phi tuyến tính\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    # Lớp cuối cùng: Phải tạo ra (OUTPUT_STEPS * N_FEATURES) giá trị\n",
    "    # Sau đó Reshape lại thành (96, 5)\n",
    "    x = layers.Dense(output_steps * output_features)(x)\n",
    "    outputs = layers.Reshape((output_steps, output_features))(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# --- Khởi tạo Model ---\n",
    "if 'X_train' in locals():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Cấu hình Transformer\n",
    "    input_shape = (INPUT_STEPS, N_FEATURES) # (192, 5)\n",
    "    \n",
    "    model = build_transformer_model(\n",
    "        input_shape=input_shape,\n",
    "        output_steps=OUTPUT_STEPS,     # 96\n",
    "        output_features=N_FEATURES,    # 5\n",
    "        head_size=64,                  # Kích thước vector đặc trưng\n",
    "        num_heads=4,                   # Số lượng đầu \"chú ý\" (càng nhiều càng soi kỹ)\n",
    "        ff_dim=128,                    # Kích thước mạng Feed Forward bên trong\n",
    "        num_transformer_blocks=2,      # Số lớp Transformer chồng lên nhau\n",
    "        mlp_units=[128],               # Lớp Dense ở cuối\n",
    "        dropout=0.1,                   # Chống overfitting\n",
    "        mlp_dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Dùng AdamW (Adam + Weight Decay) thường tốt hơn cho Transformer\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Có thể dùng AdamW nếu TF > 2.10\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "    print(\"--- Transformer Architecture ---\")\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"Lỗi: Thiếu dữ liệu X_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 9: Huấn luyện Transformer ---\n",
    "\n",
    "if 'model' in locals():\n",
    "    print(\"Bắt đầu huấn luyện Transformer...\")\n",
    "    \n",
    "    # Transformer cần batch_size lớn hơn để ổn định gradient\n",
    "    BATCH_SIZE = 128  # Có thể tăng lên 256 nếu dùng GPU Kaggle\n",
    "    EPOCHS = 50       # Transformer học nhanh, có thể tăng epoch lên\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        # Giảm learning rate nếu loss không giảm (giúp model hội tụ sâu hơn)\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    print(\"Hoàn tất huấn luyện!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 10: Đánh giá Loss và \"Accuracy\" ---\n",
    "\n",
    "if 'history' in locals():\n",
    "    print(\"--- Đánh giá kết quả huấn luyện ---\")\n",
    "\n",
    "    # 1. Trực quan hóa Loss (MSE) và MAE\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Plot Loss (MSE)\n",
    "    ax1.plot(history.history['loss'], label='Train Loss (MSE)')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "    ax1.set_title('Model Loss (Mean Squared Error)')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot \"Accuracy\" (Chúng ta dùng MAE - Mean Absolute Error)\n",
    "    # GHI CHÚ: \"Accuracy\" là metric cho bài toán Phân loại (Classification).\n",
    "    # Đối với bài toán Hồi quy (Regression) như dự đoán KPI, \n",
    "    # chúng ta dùng MAE để đo \"độ chính xác\" (sai số tuyệt đối trung bình).\n",
    "    \n",
    "    ax2.plot(history.history['mae'], label='Train MAE')\n",
    "    ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    ax2.set_title('Model \"Accuracy\" (Mean Absolute Error)')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Mean Absolute Error (MAE)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # 2. Đánh giá trên tập TEST (Dữ liệu chưa từng thấy)\n",
    "    print(\"\\n--- Đánh giá trên tập Test ---\")\n",
    "    # model.evaluate sẽ trả về [loss, metric_1, metric_2, ...]\n",
    "    # Tương ứng với compile(loss='mse', metrics=['mae'])\n",
    "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    test_loss_mse = test_results[0]\n",
    "    test_metric_mae = test_results[1]\n",
    "\n",
    "    print(f\"  Test Loss (MSE): {test_loss_mse:.6f}\")\n",
    "    print(f\"  Test MAE (Mean Absolute Error): {test_metric_mae:.6f}\")\n",
    "    \n",
    "    print(\"\\nGiải thích MAE:\")\n",
    "    print(f\"Giá trị MAE = {test_metric_mae:.6f} (trên dữ liệu đã scale từ 0-1).\")\n",
    "    print(\"Điều này có nghĩa là, trên thang 0-1, dự đoán của mô hình\")\n",
    "    print(f\"sai lệch trung bình khoảng {test_metric_mae:.6f} so với giá trị thực tế.\")\n",
    "    print(\"(Để có MAE theo đơn vị gốc (MB, Users...), chúng ta cần dùng scaler.inverse_transform)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Lỗi: Biến 'history' không tồn tại. Mô hình chưa được huấn luyện.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
